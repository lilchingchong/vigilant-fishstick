# Sharpe-first optimizer for differenced series (P_t - P_{t-1})
# Priorities: Sharpe >> positive skew (optional hard) > negative excess kurtosis (optional hard).
# ONLY stats_df['target_mean'] is forecast; σ/Σ/skew/kurt from historical returns; Σ scaled by α².

from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple
import numpy as np
import pandas as pd
from scipy import optimize, stats as scistats

# -------------------- small utils --------------------
def _softplus(x: float, beta: float = 40.0) -> float:
    b = float(beta)
    if x > 0:
        return (1.0 / b) * (np.log1p(np.exp(-b * x)) + b * x)
    else:
        return (1.0 / b) * np.log1p(np.exp(b * x))

def _softmax(z: np.ndarray) -> np.ndarray:
    z = np.asarray(z, dtype=float)
    m = np.max(z)
    e = np.exp(z - m)
    return e / np.clip(e.sum(), 1e-16, None)

def _l1_project_to_sum_abs_one(w: np.ndarray, eps: float = 1e-16) -> np.ndarray:
    s = float(np.sum(np.abs(w)))
    if s <= eps:
        n = w.shape[0]
        signs = np.where(np.arange(n) % 2 == 0, 1.0, -1.0)
        return signs / float(n)
    return w / s

# -------------------- options --------------------
@dataclass
class Opts:
    # Historical moment taper (no EWM/winsorization)
    shrinkage: float = 0.30
    moment_shrink_strength: float = 1.0
    moment_scale_clip: Tuple[float, float] = (0.6, 1.8)
    extrapolation_taper: float = 1.0
    cov_ridge_rel: float = 1e-8

    # Structure / regularization
    lam_l2: float = 0.10
    lam_concentration: float = 0.50
    lam_concentraition: Optional[float] = None  # alias accepted
    concentration_center: bool = True

    # Shape prefs (Sharpe >> skew > neg. excess kurt)
    lam_skew: float = 0.30      # soft weight (LBFGS mode)
    s_target: float = 0.0       # skew target (hard if enabled)
    lam_kurt: float = 0.10      # soft (tertiary)
    k_target: float = 0.0       # prefer exkurt <= k_target (usually 0)

    # Mode switches
    hard_skew: bool = False           # enforce skew >= s_target
    hard_kurt: bool = False           # enforce exkurt <= k_target
    set_all_constraints: bool = False # NEW: if True, enforce BOTH hard skew & hard kurt
    enforce_constraints: bool = False # legacy; if True -> hard_skew=True

    # LBFGS soft-mode nudges (ignored by SLSQP hard-constraints)
    lam_abs_sum: float = 25.0
    lam_overlap: float = 1.0

    # Solver / safety
    n_run: int = 4
    restart_seed: int = 123
    Maxiter: int = 1000
    tol: float = 1e-6
    atol: float = 1e-6
    max_fun_eval_per_run: int = 200_000

    # L-BFGS-B tuning (↑ iterations by default)
    lbfgs_ftol: float = 1e-14
    lbfgs_gtol: float = 1e-12
    lbfgs_maxls: int = 200

    # SLSQP tuning (↑ iterations by default)
    slsqp_maxiter: Optional[int] = None  # None → use Maxiter
    slsqp_ftol: Optional[float] = 1e-10  # smaller than tol for longer runs

    # Hard mode multi-start inside each run
    hard_attempts_per_run: int = 3

    # Printing (every iteration; print best Sharpe-so-far only)
    print_progress: bool = True

# -------------------- optimizer --------------------
class PortfolioOptimizer:
    def __init__(self, returns: pd.DataFrame, stats_df: pd.DataFrame, opts: Opts):
        self.R = self._sanitize_returns_df(returns)
        self.asset_names = list(self.R.columns)
        self.T, self.N = self.R.shape
        self.opts = self._normalize_opts(opts)

        # forward-looking mean (only forecasted thing)
        if 'target_mean' not in stats_df.columns:
            raise ValueError("stats_df must include 'target_mean'.")
        self.mu_f = np.asarray(stats_df['target_mean'].values, dtype=float).reshape(-1)
        if self.mu_f.shape[0] != self.N:
            raise ValueError("Length of stats_df['target_mean'] must equal number of assets (returns columns).")

        # historical moments
        self.mu_h = np.asarray(self.R.mean(axis=0).values, dtype=float)
        cov_h = np.asarray(self.R.cov(ddof=1).values, dtype=float)

        # covariance: diagonal shrink + tiny ridge
        theta = float(np.clip(self.opts.shrinkage, 0.0, 1.0))
        diag_cov = np.diag(np.diag(cov_h))
        cov_shrunk = (1.0 - theta) * cov_h + theta * diag_cov
        diag_mean = float(max(np.mean(np.diag(cov_shrunk)), 1e-16))
        cov_shrunk += float(self.opts.cov_ridge_rel) * diag_mean * np.eye(self.N, dtype=float)

        # α² scaling for Sharpe realism (means untouched)
        self.alpha_scale = self._compute_alpha_from_means(self.mu_f, self.mu_h, self.opts.moment_scale_clip)
        self.cov_eff = (self.alpha_scale ** 2) * cov_shrunk
        self.cov_eff = 0.5 * (self.cov_eff + self.cov_eff.T)  # symmetrize

        # cache & shrinkers
        self._R_vals = self.R.values
        self.skew_shrink, self.kurt_shrink = self._standardized_moment_shrink_factors(
            T=self.T, theta_diag=theta, c=self.opts.moment_shrink_strength,
            alpha=self.alpha_scale, extra=self.opts.extrapolation_taper
        )

        # per-run state
        self._feval_count = 0
        self._best_in_run = None  # dict with sharpe, w, skew, kurt

    # --------------- public API ---------------
    def solve_max_sharpe(self) -> Dict[str, Any]:
        hs, hk = self._effective_constraint_flags()
        hard_mode = bool(hs or hk)
        return self._solve_slsqp_hard() if hard_mode else self._solve_lbfgs_soft()

    # --------------- LBFGS (soft) ---------------
    def _solve_lbfgs_soft(self) -> Dict[str, Any]:
        global_best = None
        for run_idx in range(int(self.opts.n_run)):
            best = self._lbfgs_single_run(seed=int(self.opts.restart_seed) + run_idx,
                                          run_idx=run_idx,
                                          use_analytic_start=(run_idx == 0))
            if (global_best is None) or (best["sharpe"] > global_best["sharpe"]):
                global_best = {**best, "run": run_idx, "method": "L-BFGS-B (soft)"}
            if self.opts.print_progress:
                print(f"[Run {run_idx}] best Sharpe={best['sharpe']:.6f} skew={best['skew']:.6f} kurt={best['kurt']:.6f}")
        w_opt = global_best["w"]
        stats = self._portfolio_stats(w_opt)
        return self._package_result(w_opt, stats, ok=True, status="OK",
                                    method=f"{global_best['method']} (best_run={global_best['run']})")

    def _lbfgs_single_run(self, seed: int, run_idx: int, use_analytic_start: bool = True) -> Dict[str, Any]:
        self._feval_count = 0
        self._best_in_run = None

        if use_analytic_start:
            w0 = self._analytic_warm_start()
            v0 = self._w_to_simplex_uv(w0)
            z0 = np.log(v0 + 1e-12)
        else:
            rng = np.random.default_rng(seed)
            z0 = rng.normal(0.0, 0.7, size=2 * self.N)

        cb = self._make_callback_lbfgs(run_idx)
        optimize.minimize(
            fun=self._neg_objective_z,
            x0=z0,
            method="L-BFGS-B",
            callback=cb,
            options={
                "maxiter": int(self.opts.Maxiter),
                "maxls": int(self.opts.lbfgs_maxls),
                "ftol": float(self.opts.lbfgs_ftol),
                "gtol": float(self.opts.lbfgs_gtol),
                "disp": False
            }
        )

        # if no iterate recorded, fall back to initial
        if self._best_in_run is None:
            s0 = _softmax(z0)
            w0 = s0[:self.N] - s0[self.N:]
            w_best = _l1_project_to_sum_abs_one(w0)
            stats = self._portfolio_stats(w_best)
            self._best_in_run = {"sharpe": stats["model_sharpe"], "skew": stats["skew"], "kurt": stats["kurt"], "w": w_best}

        return self._best_in_run

    def _make_callback_lbfgs(self, run_idx: int):
        state = {"it": 0}
        def cb(xk: np.ndarray):
            # compute metrics at current iterate
            s = _softmax(xk)
            w = s[:self.N] - s[self.N:]
            w_proj = _l1_project_to_sum_abs_one(w)
            num = float(np.dot(self.mu_f, w_proj))
            den_q = float(np.dot(w_proj, self.cov_eff @ w_proj)); den = float(np.sqrt(max(den_q, 1e-16)))
            model_sharpe = num / den if den > 0 else -1e-12
            r_p = self._R_vals @ w_proj
            skew = self._shrunk_skew(r_p); exkurt = self._shrunk_exkurt(r_p)

            # update best-so-far if improved
            if (self._best_in_run is None) or (model_sharpe > self._best_in_run["sharpe"] + 1e-12):
                self._best_in_run = {"sharpe": float(model_sharpe), "skew": float(skew), "kurt": float(exkurt), "w": w_proj.copy()}

            # PRINT: best Sharpe-so-far EVERY iteration (monotone)
            if self.opts.print_progress:
                bs = self._best_in_run
                print(f"[Run {run_idx} | it {state['it']}] best_Sharpe={bs['sharpe']:.6f} best_skew={bs['skew']:.6f} best_kurt={bs['kurt']:.6f}")
            state["it"] += 1
        return cb

    def _neg_objective_z(self, z: np.ndarray) -> float:
        # evaluation budget guard
        self._feval_count += 1
        if self._feval_count > int(self.opts.max_fun_eval_per_run):
            return 1e6
        s = _softmax(z)
        u_plus = s[:self.N]; u_minus = s[self.N:]
        w = u_plus - u_minus
        w_proj = _l1_project_to_sum_abs_one(w)

        # Sharpe
        num = float(np.dot(self.mu_f, w_proj))
        den_q = float(np.dot(w_proj, self.cov_eff @ w_proj)); den = float(np.sqrt(max(den_q, 1e-16)))
        model_sharpe = num / den if den > 0 else -1e-12

        # penalties/regularizers (soft, secondary/tertiary)
        r_p = self._R_vals @ w_proj
        skew = self._shrunk_skew(r_p); exkurt = self._shrunk_exkurt(r_p)
        p_skew = self.opts.lam_skew * (_softplus(self.opts.s_target - skew)) ** 2
        p_kurt = self.opts.lam_kurt * (_softplus(exkurt - self.opts.k_target)) ** 2
        abs_sum = float(np.sum(np.sqrt(w * w + 1e-12)))
        p_abs = self.opts.lam_abs_sum * (abs_sum - 1.0) ** 2
        p_overlap = self.opts.lam_overlap * float(np.sum(u_plus * u_minus))
        l2_pen = self.opts.lam_l2 * float(np.dot(w_proj, w_proj))
        g = np.abs(w_proj); herf = float(np.sum(g * g)) - (1.0 / self.N if self.opts.concentration_center else 0.0)
        conc_pen = self._lam_concentration() * herf
        obj = model_sharpe - (p_skew + p_kurt + p_abs + p_overlap + l2_pen + conc_pen)

        # keep best-so-far silently (covers cases when callback is not fired)
        if (self._best_in_run is None) or (model_sharpe > self._best_in_run["sharpe"] + 1e-12):
            self._best_in_run = {"sharpe": float(model_sharpe), "skew": float(skew), "kurt": float(exkurt), "w": w_proj.copy()}
        return -obj

    # --------------- SLSQP (hard constraints) ---------------
    def _solve_slsqp_hard(self) -> Dict[str, Any]:
        """Max Sharpe subject to hard skew/kurt constraints. No soft fallback; only feasible solutions are accepted."""
        hs, hk = self._effective_constraint_flags()
        global_best = None

        for run_idx in range(int(self.opts.n_run)):
            best_feasible_this_run = None

            for attempt in range(int(self.opts.hard_attempts_per_run)):
                # start: analytic on attempt 0; random Dirichlet thereafter
                if attempt == 0:
                    w_ws = self._analytic_warm_start()
                    v0 = self._w_to_simplex_uv(w_ws)
                else:
                    rng = np.random.default_rng(int(self.opts.restart_seed) + 10_000 * run_idx + attempt)
                    v0 = rng.dirichlet(np.ones(2 * self.N))

                bounds = [(0.0, 1.0)] * (2 * self.N)
                cons = [{"type": "eq", "fun": lambda v: np.sum(v) - 1.0}]
                if hs:
                    cons.append({"type": "ineq", "fun": lambda v: self._skew_constraint_v(v)})   # skew ≥ s_target
                if hk:
                    cons.append({"type": "ineq", "fun": lambda v: self._kurt_constraint_v(v)})   # exkurt ≤ k_target

                cb = self._make_callback_slsqp(run_idx)
                res = optimize.minimize(
                    fun=self._neg_objective_v, x0=v0, method="SLSQP",
                    bounds=bounds, constraints=cons, callback=cb,
                    options={
                        "maxiter": int(self.opts.slsqp_maxiter or self.opts.Maxiter),
                        "ftol": float(self.opts.slsqp_ftol if self.opts.slsqp_ftol is not None else self.opts.tol),
                        "disp": False
                    }
                )

                # If solver produced a candidate, check feasibility explicitly on projected weights.
                if hasattr(res, "x"):
                    v = res.x
                    w = v[:self.N] - v[self.N:]
                    w_proj = _l1_project_to_sum_abs_one(w)
                    stats = self._portfolio_stats(w_proj)
                    feasible = True
                    if hs and (stats["skew"] < self.opts.s_target - 1e-6):
                        feasible = False
                    if hk and (stats["kurt"] > self.opts.k_target + 1e-6):
                        feasible = False

                    if feasible:
                        cand = {"sharpe": stats["model_sharpe"], "skew": stats["skew"], "kurt": stats["kurt"], "w": w_proj}
                        if (best_feasible_this_run is None) or (cand["sharpe"] > best_feasible_this_run["sharpe"]):
                            best_feasible_this_run = cand

                if best_feasible_this_run is not None:
                    break  # move to next run

            # Run summary
            if self.opts.print_progress:
                if best_feasible_this_run is not None:
                    print(f"[Run {run_idx}] best Sharpe={best_feasible_this_run['sharpe']:.6f} "
                          f"skew={best_feasible_this_run['skew']:.6f} kurt={best_feasible_this_run['kurt']:.6f}")
                else:
                    print(f"[Run {run_idx}] no feasible solution under hard constraints; continuing.")

            # Update global best only with feasible candidates
            if best_feasible_this_run is not None:
                if (global_best is None) or (best_feasible_this_run["sharpe"] > global_best["sharpe"]):
                    global_best = {**best_feasible_this_run, "run": run_idx, "method": "SLSQP (hard)"}

        if global_best is None:
            w_fail = self._analytic_warm_start()
            stats_fail = self._portfolio_stats(w_fail)
            return self._package_result(
                w_fail, stats_fail, ok=False,
                status="No feasible solution under hard constraints",
                method="SLSQP (hard)"
            )

        w_opt = global_best["w"]
        stats = self._portfolio_stats(w_opt)
        return self._package_result(
            w_opt, stats, ok=True, status="OK",
            method=f"{global_best['method']} (best_run={global_best['run']})"
        )

    def _make_callback_slsqp(self, run_idx: int):
        state = {"it": 0}
        def cb(xk: np.ndarray):
            v = xk
            u_plus = v[:self.N]; u_minus = v[self.N:]; w = u_plus - u_minus
            w_proj = _l1_project_to_sum_abs_one(w)
            num = float(np.dot(self.mu_f, w_proj))
            den_q = float(np.dot(w_proj, self.cov_eff @ w_proj)); den = float(np.sqrt(max(den_q, 1e-16)))
            model_sharpe = num / den if den > 0 else -1e-12
            r_p = self._R_vals @ w_proj
            skew = self._shrunk_skew(r_p); exkurt = self._shrunk_exkurt(r_p)

            # update best-so-far
            if (self._best_in_run is None) or (model_sharpe > self._best_in_run["sharpe"] + 1e-12):
                self._best_in_run = {"sharpe": float(model_sharpe), "skew": float(skew), "kurt": float(exkurt), "w": w_proj.copy()}

            # PRINT: best Sharpe-so-far EVERY iteration (monotone)
            if self.opts.print_progress:
                bs = self._best_in_run
                print(f"[Run {run_idx} | it {state['it']}] best_Sharpe={bs['sharpe']:.6f} best_skew={bs['skew']:.6f} best_kurt={bs['kurt']:.6f}")
            state["it"] += 1
        return cb

    def _neg_objective_v(self, v: np.ndarray) -> float:
        u_plus = v[:self.N]; u_minus = v[self.N:]; w = u_plus - u_minus
        w_proj = _l1_project_to_sum_abs_one(w)
        # Sharpe
        num = float(np.dot(self.mu_f, w_proj))
        den_q = float(np.dot(w_proj, self.cov_eff @ w_proj)); den = float(np.sqrt(max(den_q, 1e-16)))
        model_sharpe = num / den if den > 0 else -1e-12
        # tertiary soft kurtosis + regularizers
        r_p = self._R_vals @ w_proj
        exkurt = self._shrunk_exkurt(r_p)
        p_kurt = self.opts.lam_kurt * (_softplus(exkurt - self.opts.k_target)) ** 2
        l2_pen = self.opts.lam_l2 * float(np.dot(w_proj, w_proj))
        g = np.abs(w_proj); herf = float(np.sum(g * g)) - (1.0 / self.N if self.opts.concentration_center else 0.0)
        conc_pen = self._lam_concentration() * herf
        obj = model_sharpe - (p_kurt + l2_pen + conc_pen)

        # keep best-so-far silently (in case callback not invoked)
        r_skew = self._shrunk_skew(r_p)
        if (self._best_in_run is None) or (model_sharpe > self._best_in_run["sharpe"] + 1e-12):
            self._best_in_run = {"sharpe": float(model_sharpe), "skew": float(r_skew), "kurt": float(exkurt), "w": w_proj.copy()}
        return -obj

    # ---- hard constraints ----
    def _skew_constraint_v(self, v: np.ndarray) -> float:
        u_plus = v[:self.N]; u_minus = v[self.N:]; w = u_plus - u_minus
        w_proj = _l1_project_to_sum_abs_one(w)
        r_p = self._R_vals @ w_proj
        skew = self._shrunk_skew(r_p)
        return float(skew - self.opts.s_target + self.opts.atol)      # skew >= s_target

    def _kurt_constraint_v(self, v: np.ndarray) -> float:
        u_plus = v[:self.N]; u_minus = v[self.N:]; w = u_plus - u_minus
        w_proj = _l1_project_to_sum_abs_one(w)
        r_p = self._R_vals @ w_proj
        exkurt = self._shrunk_exkurt(r_p)
        return float(self.opts.k_target - exkurt + self.opts.atol)    # exkurt <= k_target

    # --------------- stats & shrink ---------------
    def _portfolio_stats(self, w: np.ndarray) -> Dict[str, float]:
        std_vec = np.sqrt(np.clip(np.diag(self.cov_eff), 1e-32, None))
        den_q = float(np.dot(w, self.cov_eff @ w)); den = float(np.sqrt(max(den_q, 1e-16)))
        num = float(np.dot(self.mu_f, w))
        model_sharpe = num / den if den > 0 else np.nan

        r_p = self._R_vals @ w
        r_mean = float(np.mean(r_p)) if r_p.size else np.nan
        r_std = float(np.std(r_p, ddof=1)) if r_p.size > 1 else np.nan
        realized_sharpe = r_mean / r_std if (np.isfinite(r_std) and r_std > 0) else np.nan

        skew = self._shrunk_skew(r_p)
        exkurt = self._shrunk_exkurt(r_p)
        return {"mean_vec": self.mu_f.copy(), "std": std_vec, "model_sharpe": model_sharpe,
                "realized_sharpe": realized_sharpe, "skew": skew, "kurt": exkurt}

    def _shrunk_skew(self, r: np.ndarray) -> float:
        if r.size < 3:
            return 0.0
        s = float(scistats.skew(r, bias=False))
        if not np.isfinite(s):
            return 0.0
        return float(self.skew_shrink * s)

    def _shrunk_exkurt(self, r: np.ndarray) -> float:
        if r.size < 4:
            return 0.0
        k = float(scistats.kurtosis(r, fisher=True, bias=False))
        if not np.isfinite(k):
            return 0.0
        return float(self.kurt_shrink * k)

    def _standardized_moment_shrink_factors(self, T: int, theta_diag: float, c: float, alpha: float, extra: float) -> Tuple[float, float]:
        if T < 4:
            return 0.0, 0.0
        se2_skew = 6.0 / max(T, 1.0)
        se2_kurt = 24.0 / max(T, 1.0)
        ridge_skew = 1.0 / (1.0 + max(c, 0.0) * se2_skew)
        ridge_kurt = 1.0 / (1.0 + max(c, 0.0) * se2_kurt)
        g_alpha = 1.0 / (1.0 + max(extra, 0.0) * (alpha - 1.0) ** 2)
        base = (1.0 - np.clip(theta_diag, 0.0, 1.0))
        return float(base * ridge_skew * g_alpha), float(base * ridge_kurt * g_alpha)

    # --------------- fallbacks & helpers ---------------
    def _analytic_warm_start(self) -> np.ndarray:
        """Σ^{-1} μ, then scale to ∑|w|=1 (ridge-safe)."""
        try:
            A = self.cov_eff + 1e-12 * np.eye(self.N)
            w = np.linalg.solve(A, self.mu_f)
        except np.linalg.LinAlgError:
            w = np.linalg.pinv(self.cov_eff) @ self.mu_f
        return _l1_project_to_sum_abs_one(w)

    def _w_to_simplex_uv(self, w: np.ndarray) -> np.ndarray:
        """Map signed w to (u+,u-) simplex vector."""
        u_plus = np.clip(w, 0.0, None)
        u_minus = np.clip(-w, 0.0, None)
        v = np.concatenate([u_plus, u_minus])
        s = v.sum()
        return (np.ones_like(v) / float(v.size)) if s <= 0 else (v / s)

    def _lam_concentration(self) -> float:
        return float(self.opts.lam_concentration if self.opts.lam_concentration is not None
                     else (self.opts.lam_concentraition if self.opts.lam_concentraition is not None else 0.0))

    def _compute_alpha_from_means(self, mu_f: np.ndarray, mu_h: np.ndarray, clip_bounds: Tuple[float, float]) -> float:
        lo, hi = clip_bounds
        if hi <= lo:
            raise ValueError("moment_scale_clip must satisfy lo < hi.")
        eps = 1e-16
        med_h = float(np.median(np.abs(mu_h)))
        denom = med_h if med_h > eps else eps
        ratio = float(np.median(np.abs(mu_f))) / denom
        return float(np.clip(ratio, lo, hi))

    def _normalize_opts(self, opts: Opts) -> Opts:
        # support alias
        if getattr(opts, "lam_concentraition", None) is not None:
            opts.lam_concentration = float(opts.lam_concentraition)
        # legacy flag
        if getattr(opts, "enforce_constraints", False):
            opts.hard_skew = True
        return opts

    def _effective_constraint_flags(self) -> Tuple[bool, bool]:
        """Combine flags so set_all_constraints enforces BOTH hard constraints."""
        hs = bool(self.opts.hard_skew or self.opts.enforce_constraints or self.opts.set_all_constraints)
        hk = bool(self.opts.hard_kurt or self.opts.set_all_constraints)
        return hs, hk

    @staticmethod
    def _sanitize_returns_df(df_like) -> pd.DataFrame:
        if not isinstance(df_like, pd.DataFrame):
            df = pd.DataFrame(df_like)
        else:
            df = df_like.copy()
        for c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
        df = df.dropna(axis=1, how="all").dropna(axis=0, how="any")
        if df.shape[1] == 0 or df.shape[0] < 2:
            raise ValueError("returns must contain at least 2 observations and 1 numeric column after cleaning.")
        return df

    def _package_result(self, w_opt: np.ndarray, stats: Dict[str, Any], ok: bool, status: str, method: str) -> Dict[str, Any]:
        gross_ok = np.isclose(np.sum(np.abs(w_opt)), 1.0, atol=self.opts.atol)
        hs, hk = self._effective_constraint_flags()
        skew_ok = (not hs) or (stats["skew"] >= self.opts.s_target - 1e-6)
        kurt_ok = (not hk) or (stats["kurt"] <= self.opts.k_target + 1e-6)
        return {
            "ok": bool(ok and gross_ok and skew_ok and kurt_ok),
            "status": status,                    # "OK" or infeasible message
            "method": method,
            "weights": pd.Series(w_opt, index=self.asset_names),
            "mean": stats["mean_vec"],
            "std": stats["std"],
            "model_sharpe": stats["model_sharpe"],
            "realized_sharpe": stats["realized_sharpe"],
            "skew": stats["skew"],
            "kurt": stats["kurt"],
            "gross_exposure": float(np.sum(np.abs(w_opt))),
            "max_position": float(np.max(np.abs(w_opt))),
            "n_active_assets": int(np.sum(np.abs(w_opt) > 1e-8)),
            "constraints_satisfied": bool(gross_ok and skew_ok and kurt_ok),
            "skew_positive": bool(stats["skew"] >= 0.0),
            "kurt_negative": bool(stats["kurt"] <= 0.0),
        }
